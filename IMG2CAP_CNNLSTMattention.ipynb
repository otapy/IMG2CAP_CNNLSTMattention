{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxyg2lPaU6eU"
      },
      "outputs": [],
      "source": [
        "#ライブラリ\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from pycocotools.coco import COCO\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in8Tt9_UU9NJ"
      },
      "outputs": [],
      "source": [
        "#COCOデータダウンロード\n",
        "!mkdir data\n",
        "!wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data/\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip -P ./data/\n",
        "\n",
        "!unzip ./data/captions_train-val2014.zip -d ./data/\n",
        "!rm ./data/captions_train-val2014.zip\n",
        "!unzip ./data/train2014.zip -d ./data/\n",
        "!rm ./data/train2014.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9p9U0GKVALR"
      },
      "outputs": [],
      "source": [
        "#ボキャブラリー作成\n",
        "class Vocabulary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "def make_vocab(json, threshold):\n",
        "    coco = COCO(json)\n",
        "    counter = Counter()\n",
        "    ids = coco.anns.keys()\n",
        "    for i, id in enumerate(ids):\n",
        "        caption = str(coco.anns[id]['caption'])\n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "        counter.update(tokens)\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(\"[{}/{}] Tokenized captions.\".format(i+1, len(ids)))\n",
        "\n",
        "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
        "\n",
        "    vocab = Vocabulary()\n",
        "    vocab.add_word('<pad>')\n",
        "    vocab.add_word('<start>')\n",
        "    vocab.add_word('<end>')\n",
        "    vocab.add_word('<unk>')\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        vocab.add_word(word)\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzm_3Dh7VDmN"
      },
      "outputs": [],
      "source": [
        "#vocabインスタンス\n",
        "vocab = make_vocab(json='./data/annotations/captions_train2014.json', threshold=4)\n",
        "vocab_path = './data/vocab.pkl'\n",
        "with open(vocab_path, 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
        "print(\"Saved vocabulary wrapper to '{}'\".format(vocab_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsaQlMsgVC0I"
      },
      "outputs": [],
      "source": [
        "#COCO画像リサイズ\n",
        "def resize_image(image, size):\n",
        "    return image.resize((size,size), Image.ANTIALIAS)\n",
        "\n",
        "def resize_images(image_dir, output_dir, size):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    images = os.listdir(image_dir)\n",
        "    num_images = len(images)\n",
        "    for i, image in enumerate(images):\n",
        "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
        "            with Image.open(f) as img:\n",
        "                img = resize_image(img, size)\n",
        "                img.save(os.path.join(output_dir, image), img.format)\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
        "                   .format(i+1, num_images, output_dir))\n",
        "\n",
        "image_dir = './data/train2014/'\n",
        "output_dir = './data/resized2014/'\n",
        "image_size = 256\n",
        "resize_images(image_dir, output_dir, image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZJcZwH_VMm6"
      },
      "outputs": [],
      "source": [
        "#COCOカスタムデータセット\n",
        "class CocoDataset(data.Dataset):\n",
        "    def __init__(self, root, json, vocab, transform=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(json)\n",
        "        self.ids = list(self.coco.anns.keys())\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        vocab = self.vocab\n",
        "        ann_id = self.ids[index]\n",
        "        caption = coco.anns[ann_id]['caption']\n",
        "        img_id = coco.anns[ann_id]['image_id']\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "        caption = []\n",
        "        caption.append(vocab('<start>'))\n",
        "        caption.extend([vocab(token) for token in tokens])\n",
        "        caption.append(vocab('<end>'))\n",
        "        target = torch.Tensor(caption)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH8EEQJbVO4c"
      },
      "outputs": [],
      "source": [
        "#バッチ\n",
        "def collate_fn(data):\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = cap[:end]        \n",
        "    return images, targets\n",
        "\n",
        "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
        "    coco = CocoDataset(root=root,\n",
        "                       json=json,\n",
        "                       vocab=vocab,\n",
        "                       transform=transform)\n",
        "    \n",
        "    data_loader = torch.utils.data.DataLoader(dataset=coco, \n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=shuffle,\n",
        "                                              num_workers=num_workers,\n",
        "                                              collate_fn=collate_fn,\n",
        "                                              drop_last=True)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MnS7DjfVRBy"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_size, encoder_out_size=2048, encode_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear_hc = nn.Linear(encoder_out_size, hidden_size)\n",
        "        self.encode_size = encode_size\n",
        "        self.encoder_out_size = encoder_out_size\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = F.adaptive_avg_pool2d(features, (self.encode_size, self.encode_size)) \n",
        "        features = features.view(features.shape[0], -1, self.encoder_out_size)\n",
        "        mean_features = features.mean(dim=1)\n",
        "        h = self.linear_hc(mean_features)\n",
        "        c = self.linear_hc(mean_features)\n",
        "        return features, h, c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CellDecoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, batch_size, encoder_out_size=2048, encode_size=14):\n",
        "        super(CellDecoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstmcell = nn.LSTMCell(embed_size, hidden_size, bias=True)\n",
        "        self.linear_ft = nn.Linear(encoder_out_size, hidden_size)\n",
        "        self.linear_h = nn.Linear(encode_size*encode_size, hidden_size)\n",
        "        self.linear_word = nn.Linear(hidden_size, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, captions, features, init_h, init_c, padmask=None):\n",
        "        h = init_h\n",
        "        c = init_c\n",
        "        embeddings = self.embed(captions)\n",
        "        features = self.linear_ft(features)\n",
        "        text = torch.zeros(self.batch_size, embeddings.shape[1], self.vocab_size, device=device)\n",
        "        for i in range(embeddings.shape[1]): \n",
        "              scores = torch.matmul(features, h.unsqueeze(1).transpose(-2, -1))\n",
        "              scores = scores.masked_fill(padmask[:,:,i].unsqueeze(1) == 0, -1e9)\n",
        "              atten_weights = F.softmax(scores, dim=-1) \n",
        "              weights_features = h.unsqueeze(1) * atten_weights\n",
        "              h = torch.matmul(weights_features, h.unsqueeze(1).transpose(-2, -1))\n",
        "              h = self.linear_h(h.squeeze(2))\n",
        "              h, c = self.lstmcell(embeddings[:,i,:], (h, c))\n",
        "              word = self.linear_word(F.dropout(h))\n",
        "              text[:,i,:] = word\n",
        "        return text"
      ],
      "metadata": {
        "id": "wFHdj9dXQ1cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FS3RY2UVTR-"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, batch_size, encoder_out_size=2048, decoder_out_size=512, atten_size=512):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear_q = nn.Linear(decoder_out_size, atten_size)\n",
        "        self.linear_k = nn.Linear(encoder_out_size, atten_size)\n",
        "        self.linear = nn.Linear((hidden_size+atten_size), vocab_size)\n",
        "        #self.linear = nn.Linear(14*14, vocab_size)                  #another\n",
        "        self.embed_size = embed_size\n",
        "        self.atten_size = atten_size\n",
        "        self.encoder_out_size = encoder_out_size\n",
        "        \n",
        "    def forward(self, captions, features, h, c, padmask=None):\n",
        "        embeddings = self.embed(captions)\n",
        "        hiddens, state = self.lstm(embeddings, (h, c))\n",
        "        atten_hiddens = self.linear_q(hiddens)\n",
        "        atten_features = self.linear_k(features)\n",
        "        scores = torch.matmul(atten_features, atten_hiddens.transpose(-2, -1))\n",
        "        scores = scores.masked_fill(padmask == 0, -1e9)\n",
        "        atten_weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.zeros(batch_size, 1, self.atten_size, device=device)\n",
        "        for i in range(atten_weights.size()[2]):\n",
        "            temp_weights = atten_weights[:,:,i].unsqueeze(2)\n",
        "            weights_features = atten_features * temp_weights\n",
        "            weights_features_sum = torch.sum(weights_features, axis=1).unsqueeze(1)\n",
        "            context = torch.cat([context, weights_features_sum], dim=1)\n",
        "        context = context[:,1:,:]\n",
        "        hiddens = torch.cat([hiddens, context], dim=2)\n",
        "        #hiddens = torch.matmul(context, features.transpose(-2,-1))  #another\n",
        "        hiddens = self.linear(hiddens)\n",
        "        return hiddens, state, atten_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W68OCnDWVV2i"
      },
      "outputs": [],
      "source": [
        "#学習\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = 'modelstest/'\n",
        "crop_size = 224\n",
        "vocab_path = 'data/vocab.pkl'\n",
        "image_dir ='data/resized2014'\n",
        "caption_path='data/annotations/captions_train2014.json'\n",
        "log_step=10\n",
        "save_step=100\n",
        "embed_size=512\n",
        "hidden_size=512\n",
        "num_layers=1\n",
        "num_epochs=100\n",
        "batch_size=64\n",
        "num_workers=2\n",
        "learning_rate=0.0001\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "transform = transforms.Compose([ \n",
        "    transforms.RandomCrop(crop_size),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "data_loader = get_loader(image_dir, caption_path, vocab, transform, batch_size,\n",
        "                          shuffle=True, num_workers=num_workers) \n",
        "\n",
        "encoder = Encoder(hidden_size).to(device)\n",
        "decoder = CellDecoder(embed_size, hidden_size, len(vocab), batch_size).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "total_step = len(data_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, captions) in enumerate(data_loader):\n",
        "        \n",
        "        images = images.to(device)\n",
        "\n",
        "        caption = captions[:, :-1]\n",
        "        targets = captions[:, 1:]\n",
        "\n",
        "        caption = caption.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        pad = 0\n",
        "        pad_mask = (targets != pad).unsqueeze(1)\n",
        "        \n",
        "        feature, h, c = encoder(images)\n",
        "\n",
        "        outputs = decoder(caption, feature, h, c, pad_mask)\n",
        "\n",
        "        #loss = 0\n",
        "        #for j in range(outputs.size()[1]):\n",
        "        #    loss += criterion(outputs[:, j, :], targets[:, j])\n",
        "\n",
        "        loss = criterion(outputs.reshape(-1, outputs.shape[-1]), targets.reshape(-1))\n",
        "\n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % log_step == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
        "                  .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item())))\n",
        "        \n",
        "        all_losses.append(loss.item())\n",
        "            \n",
        "        if (i+1) % save_step == 0:\n",
        "            torch.save(encoder.state_dict(), os.path.join(\n",
        "                model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "        if (i+1) % save_step == 0:\n",
        "            torch.save(decoder.state_dict(), os.path.join(\n",
        "                model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "            \n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw0RuD0PVZOY"
      },
      "outputs": [],
      "source": [
        "#予測\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = 'modelstest/'\n",
        "vocab_path = 'data/vocab.pkl'\n",
        "image_dir ='data/resized2014'\n",
        "caption_path='data/annotations/captions_train2014.json'\n",
        "embed_size=512\n",
        "hidden_size=512\n",
        "num_layers=1\n",
        "batch_size=64\n",
        "\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "encoder = Encoder(hidden_size).to(device)\n",
        "decoder = CellDecoder(embed_size, hidden_size, len(vocab), batch_size).to(device)\n",
        "encoder.load_state_dict(torch.load('./modelstest/encoder-1-600.ckpt',torch.device('cpu')))\n",
        "decoder.load_state_dict(torch.load('./modelstest/decoder-1-600.ckpt',torch.device('cpu')))\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "test_img_dir = './data/train2014/COCO_train2014_000000000025.jpg'\n",
        "test_img = Image.open(test_img_dir).convert('RGB')\n",
        "test_img = test_img.resize([256, 256], Image.LANCZOS)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "test_img = transform(test_img)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#予測(CellDecoder)\n",
        "images = test_img.to(device)\n",
        "predicted_ids = []\n",
        "caption = []\n",
        "start_char = [vocab('<start>') for _ in range(batch_size)]\n",
        "input_char = torch.tensor(start_char, device=device)\n",
        "input_char = input_char.unsqueeze(1)\n",
        "images = images.unsqueeze(0)\n",
        "images = images.repeat(batch_size,1,1,1)\n",
        "with torch.no_grad():\n",
        "    features, h, c = encoder(images)\n",
        "    for i in range(20):\n",
        "        with torch.no_grad():\n",
        "            pad = 0\n",
        "            pad_mask = (input_char != pad).unsqueeze(1)\n",
        "            outputs = decoder(input_char, features, h, c, pad_mask)\n",
        "            _, output_chars = torch.max(outputs,dim=2)\n",
        "            if int(output_chars[0]) == vocab('<end>'):\n",
        "                break\n",
        "            output_char = output_chars[:,-1]\n",
        "            predicted_ids.append(int(output_char[0]))\n",
        "            input_char = output_char.unsqueeze(1)\n",
        "\n",
        "for j in range(len(predicted_ids)):\n",
        "    word = vocab.idx2word[int(predicted_ids[j])]\n",
        "    caption.append(word)"
      ],
      "metadata": {
        "id": "247O3VwqPXsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tICAp97lVfkd"
      },
      "outputs": [],
      "source": [
        "#予測(Decoder)\n",
        "images = test_img.to(device)\n",
        "predicted_ids = []\n",
        "caption = []\n",
        "start_char = [vocab('<start>') for _ in range(batch_size)]\n",
        "input_char = torch.tensor(start_char, device=device)\n",
        "input_char = input_char.unsqueeze(1)\n",
        "images = images.unsqueeze(0)\n",
        "images = images.repeat(batch_size,1,1,1)\n",
        "with torch.no_grad():\n",
        "    features, h, c = encoder(images)\n",
        "    for i in range(2):\n",
        "        with torch.no_grad():\n",
        "            pad = 0\n",
        "            pad_mask = (input_char != pad).unsqueeze(1)\n",
        "            outputs, states, _ = decoder(input_char, features, h, c, pad_mask)\n",
        "            h = states[0]\n",
        "            c = states[1]\n",
        "            _, output_chars = torch.max(outputs,dim=2)\n",
        "            if int(output_chars[0]) == vocab('<end>'):\n",
        "                break\n",
        "            output_char = output_chars[:,-1]\n",
        "            predicted_ids.append(int(output_char[0]))\n",
        "            input_char = output_char.unsqueeze(1)\n",
        "\n",
        "for j in range(len(predicted_ids)):\n",
        "    word = vocab.idx2word[int(predicted_ids[j])]\n",
        "    caption.append(word)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}